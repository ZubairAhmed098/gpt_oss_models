{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7920,
     "status": "ok",
     "timestamp": 1755259142862,
     "user": {
      "displayName": "zubair ahmed",
      "userId": "01234161710069653460"
     },
     "user_tz": -60
    },
    "id": "8ScHdL7Xkl-b"
   },
   "outputs": [],
   "source": [
    "# 1) Clean install (no flash-attn)\n",
    "!pip -q install -U \"transformers>=4.43\" \"accelerate>=0.33\" bitsandbytes\n",
    "\n",
    "# If you previously installed flash-attn, remove it to stop import attempts\n",
    "!pip -q uninstall -y flash-attn flash-attn-xformers 2>/dev/null || true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247,
     "referenced_widgets": [
      "ac3602f8a39846b98e5463351501f684",
      "539d43bd24bd455ba710b68cc038bbde",
      "194a1ffffdcb469ea8903312df794c1a",
      "cab012b64c14489696a0ab618387b320",
      "8792f2ca3a964f3c91168ba5217b4560",
      "49da6c1027e440538afa98cb6cc3cccf",
      "b320ba19e5e645909fa89af86ee58aca",
      "4541e4d66f964a2eb1d271bf9a9d5661",
      "5119651c79a7403daa13c43b7d2fe12b",
      "b63b7fa000fd4a289ca7552a08f06a38",
      "83986f03864b4d01991126f65939e274",
      "4e87cd91f44241f88960c98ba7ac1d92",
      "bf4ceef64f2d47bfbc4106e559250459",
      "d37c5ab45d2b4dfa989c307865e95d91",
      "64c398ea8ec64099b4c03e1789239486",
      "f787d65d2ded4eb4b1baf3b8d7b365dc",
      "b61b46ba1e1041ac8c5dc660356796d6",
      "6e1d8320f61b4f5881ae11b474aacc08",
      "f8a5ae558e0345c28e37ee5a0827d917",
      "42b98f13235f4a53b7cbf46402a6d755",
      "78366ad0fe514fb7a5a184aeaccdc845",
      "f6ad30c0d8bf4b39a1b8458084a68eb8"
     ]
    },
    "executionInfo": {
     "elapsed": 899339,
     "status": "ok",
     "timestamp": 1755260143519,
     "user": {
      "displayName": "zubair ahmed",
      "userId": "01234161710069653460"
     },
     "user_tz": -60
    },
    "id": "StfKbdT2mnEY",
    "outputId": "2ca893f1-d357-48a2-fd8f-6c0371cd2505"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3602f8a39846b98e5463351501f684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e87cd91f44241f88960c98ba7ac1d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisUser wants explanation of quantum mechanics clearly and concisely. Provide overview: basic principles, wave-particle duality, superposition, uncertainty, measurement, entanglement, applications. Keep concise but clear. Probably 3-5 paragraphs. Let's do.assistantfinal**Quantum mechanics in a nutshell**\n",
      "\n",
      "| Core idea | What it means | Why it matters |\n",
      "|-----------|---------------|----------------|\n",
      "| **Wave‑particle duality** | Light and matter behave as both waves and particles. The same entity can interfere like a wave (double‑slit experiment) and yet be counted as discrete “quanta” (photons, electrons). | Explains phenomena that classical physics can’t (e.g., photoelectric effect, electron diffraction). |\n",
      "| **Superposition** | A quantum system can exist in multiple states at once. Mathematically, its state is a linear combination of basis states. | Enables parallel computation and interference patterns; the “cat” can be alive and dead until observed. |\n",
      "| **Uncertainty principle** | Certain pairs of properties (position ↔ momentum, energy ↔ time) cannot both be known exactly; the more precisely one is known, the fuzzier the other becomes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"  # optional\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "offload_folder = \"/content/offload\"\n",
    "os.makedirs(offload_folder, exist_ok=True)\n",
    "\n",
    "# ---- Key fix: use integer device index for accelerate's max_memory ----\n",
    "if torch.cuda.is_available():\n",
    "    max_memory = {\n",
    "        0: \"28GiB\",     # NOT \"cuda:0\" — use integer index 0\n",
    "        \"cpu\": \"48GiB\",\n",
    "    }\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    # CPU-only fallback (you can also add 'disk' if RAM is limited)\n",
    "    max_memory = {\n",
    "        \"cpu\": \"64GiB\",\n",
    "        \"disk\": \"128GiB\",  # enables offload to disk if needed\n",
    "    }\n",
    "    torch_dtype = torch.bfloat16  # or torch.float32 if bf16 is not supported\n",
    "\n",
    "load_kwargs = dict(\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    offload_folder=offload_folder,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prefer PyTorch SDPA attention (no flash-attn). If not supported, drop the arg.\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        **load_kwargs,\n",
    "    )\n",
    "except TypeError:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)\n",
    "\n",
    "# ---- Chat formatting + generation ----\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Send tensors to whatever device the model’s first layer lives on\n",
    "device = next(iter(model.hf_device_map.values())) if hasattr(model, \"hf_device_map\") else 0\n",
    "inputs = {k: v.to(model.device if hasattr(model, \"device\") else device) for k, v in inputs.items()}\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        streamer=streamer,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNSBy91xzANiZgF6wnE1gJ3",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
